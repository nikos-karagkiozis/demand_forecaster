# Dockerfile.serve - custom prediction container for Vertex AI Endpoints

FROM python:3.12-slim AS builder
WORKDIR /app
RUN pip install poetry
RUN poetry config virtualenvs.in-project true
COPY pyproject.toml poetry.lock* ./
RUN poetry install --no-root --only main

FROM python:3.12-slim AS final
WORKDIR /app
# System dependencies needed at runtime (e.g., LightGBM needs libgomp)
RUN apt-get update \
    && apt-get install -y --no-install-recommends libgomp1 \
    && rm -rf /var/lib/apt/lists/*
COPY --from=builder /app/.venv ./.venv
ENV PATH="/app/.venv/bin:$PATH"
COPY src/ ./src/
ENV PYTHONPATH="/app/src:/app"

# Vertex probes port 8080 by default
ENV AIP_HTTP_PORT=8080

# Start FastAPI app (sales_forecast.serve_app:app) with uvicorn
EXPOSE 8080
CMD ["uvicorn", "sales_forecast.serve_app:app", "--host", "0.0.0.0", "--port", "8080"]


