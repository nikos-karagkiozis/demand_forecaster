{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Forecast Demo\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Querying recent data from BigQuery\n",
        "- Calling the deployed Vertex AI Endpoint for online prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.cloud import bigquery, aiplatform\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ID = os.environ.get(\"PROJECT_ID\", \"my-forecast-project-18870\")\n",
        "REGION = os.environ.get(\"REGION\", \"us-central1\")\n",
        "DATASET = os.environ.get(\"DATASET\", \"gk2_takeaway_sales\")\n",
        "FINAL_TABLE = os.environ.get(\"FINAL_TABLE\", \"daily_sales_features\")\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "df = client.query(f\"\"\"\n",
        "SELECT * FROM `{PROJECT_ID}.{DATASET}.{FINAL_TABLE}`\n",
        "ORDER BY Date DESC\n",
        "LIMIT 30\n",
        "\"\"\").to_dataframe()\n",
        "df.tail(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the deployed endpoint (set ENDPOINT_ID env var beforehand)\n",
        "ENDPOINT_ID = os.environ.get(\"ENDPOINT_ID\")\n",
        "assert ENDPOINT_ID, \"Set ENDPOINT_ID in environment to your deployed endpoint id\"\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "endpoint_name = ENDPOINT_ID if ENDPOINT_ID.startswith(\"projects/\") else f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}\"\n",
        "endpoint = aiplatform.Endpoint(endpoint_name)\n",
        "\n",
        "# Construct a single instance (dummy example, replace features accordingly)\n",
        "instance = {\"feature1\": 1.0, \"feature2\": 2.0}\n",
        "\n",
        "pred = endpoint.predict([instance])\n",
        "pred.predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on last 20% (validation) via Vertex Endpoint\n",
        "\n",
        "This section:\n",
        "- Loads the full table from BigQuery\n",
        "- Rebuilds features exactly as in training\n",
        "- Splits by time (80/20) to get the same validation window\n",
        "- Sends the validation features to the deployed Endpoint in batches\n",
        "- Computes RMSE/MAE/RÂ² against ground truth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import math\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery, aiplatform\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "try:\n",
        "    from sklearn.metrics import root_mean_squared_error\n",
        "except Exception:  # older sklearn fallback\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    def root_mean_squared_error(y_true, y_pred):\n",
        "        return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Make project modules importable\n",
        "repo_root = pathlib.Path.cwd()\n",
        "src_path = repo_root / \"src\"\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.append(str(src_path))\n",
        "\n",
        "from sales_forecast.config import config\n",
        "from sales_forecast.features import generate_features, prepare_dataset_for_modeling\n",
        "\n",
        "PROJECT_ID = os.environ.get(\"PROJECT_ID\", config.bq.PROJECT_ID)\n",
        "REGION = os.environ.get(\"REGION\", \"us-central1\")\n",
        "DATASET = os.environ.get(\"DATASET\", config.bq.DATASET)\n",
        "FINAL_TABLE = os.environ.get(\"FINAL_TABLE\", config.bq.FINAL_TABLE)\n",
        "\n",
        "# Prefer env ENDPOINT_ID; fallback to provided id\n",
        "ENDPOINT_ID = os.environ.get(\"ENDPOINT_ID\", \"5630246102608379904\")\n",
        "assert ENDPOINT_ID, \"Set ENDPOINT_ID or provide it inline\"\n",
        "\n",
        "# Initialize clients\n",
        "bq = bigquery.Client(project=PROJECT_ID)\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "endpoint_name = ENDPOINT_ID if ENDPOINT_ID.startswith(\"projects/\") else f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}\"\n",
        "endpoint = aiplatform.Endpoint(endpoint_name)\n",
        "\n",
        "# Load the full table (ascending by Date for feature generation)\n",
        "raw_df = bq.query(f\"\"\"\n",
        "SELECT * FROM `{PROJECT_ID}.{DATASET}.{FINAL_TABLE}`\n",
        "ORDER BY Date ASC\n",
        "\"\"\").to_dataframe()\n",
        "raw_df[\"Date\"] = pd.to_datetime(raw_df[\"Date\"])  # ensure dtype\n",
        "raw_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rebuild features exactly as in training\n",
        "featured_df = generate_features(raw_df)\n",
        "\n",
        "# Time-based 80/20 split to replicate training's validation window\n",
        "split_index = int(len(featured_df) * 0.8)\n",
        "val_df = featured_df.iloc[split_index:].copy()\n",
        "\n",
        "# Build model inputs (X) and ground truth (y)\n",
        "X_val, y_val = prepare_dataset_for_modeling(val_df, config.features.TARGET_COL)\n",
        "X_val = X_val.reset_index(drop=True)\n",
        "y_val = y_val.reset_index(drop=True)\n",
        "\n",
        "len(featured_df), len(X_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert validation features to JSON-serializable instances\n",
        "\n",
        "def to_native(v):\n",
        "    if isinstance(v, (np.floating,)):\n",
        "        return float(v)\n",
        "    if isinstance(v, (np.integer,)):\n",
        "        return int(v)\n",
        "    return v\n",
        "\n",
        "instances: List[Dict[str, Any]] = [\n",
        "    {k: to_native(v) for k, v in row.items()}\n",
        "    for row in X_val.to_dict(orient=\"records\")\n",
        "]\n",
        "len(instances), list(instances[0].keys())[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict in batches to avoid request size limits\n",
        "\n",
        "def predict_in_batches(endpoint, instances: List[Dict[str, Any]], batch_size: int = 100) -> List[float]:\n",
        "    preds: List[float] = []\n",
        "    for i in range(0, len(instances), batch_size):\n",
        "        chunk = instances[i:i+batch_size]\n",
        "        resp = endpoint.predict(chunk)\n",
        "        # resp.predictions is typically a list of scalars or lists\n",
        "        for p in resp.predictions:\n",
        "            # handle [value] vs value\n",
        "            if isinstance(p, list) and len(p) == 1:\n",
        "                preds.append(float(p[0]))\n",
        "            else:\n",
        "                preds.append(float(p))\n",
        "    return preds\n",
        "\n",
        "preds = predict_in_batches(endpoint, instances, batch_size=100)\n",
        "len(preds), preds[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute metrics against ground truth (validation window)\n",
        "import pandas as pd\n",
        "\n",
        "assert len(preds) == len(y_val), \"Mismatch between predictions and labels\"\n",
        "rmse = root_mean_squared_error(y_val.values, np.array(preds))\n",
        "mae = mean_absolute_error(y_val.values, np.array(preds))\n",
        "r2 = r2_score(y_val.values, np.array(preds))\n",
        "\n",
        "print({\"rmse\": float(rmse), \"mae\": float(mae), \"r2\": float(r2)})\n",
        "\n",
        "# Preview a few rows\n",
        "preview = pd.DataFrame({\n",
        "    \"Date\": val_df[\"Date\"].reset_index(drop=True),\n",
        "    \"actual\": y_val,\n",
        "    \"pred\": preds,\n",
        "}).tail(10)\n",
        "preview\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
